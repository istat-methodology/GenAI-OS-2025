{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCUOL5_m7YjB"
      },
      "outputs": [],
      "source": [
        "%pip install transformers bitsandbytes accelerate datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative LLMs for Text Classification\n",
        "Here, we implement a simple pipeline to use Generative LLMs for generic text classification tasks. We will focus on explainability, directly asking the model to explain its answers."
      ],
      "metadata": {
        "id": "qwDoltNl8nCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "RaIl_Spx882d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "Let's load the LLM from Hugging Face. We will use Google's Gemma 3 (1B) instruction tuned model. We load the Unsloth version since it doesn't require user authentication, but it's the same model."
      ],
      "metadata": {
        "id": "DmjbQKtk9CN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = pipeline(\"text-generation\", model=\"unsloth/gemma-3-1b-it\", device=device)"
      ],
      "metadata": {
        "id": "1S9VuKzN9Qg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts\n",
        "Let's define different prompts to see how they affect the model's behaviour. We will start with a **zero-shot** prompt, i.e., asking the instruction tuned model to perform the task without any examples or fine-tuning."
      ],
      "metadata": {
        "id": "RBI8LjuS7dSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_prompt = \"\"\"Classify the emotion of this Text:\n",
        "\n",
        "\"{text}\"\n",
        "\n",
        "Only use one of the following classes: [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"].\n",
        "Classify the emotion using the \"EMOTION\" tag and provide a concise explanation using the \"EXPLANATION\" tag. Pay particular attention to sarcasm and irony when classifying.\"\"\""
      ],
      "metadata": {
        "id": "172SgsjP7s9f"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's write a **few-shot** prompt, i.e., in addition to asking the model to perform the task, we include a few examples of the task being performed."
      ],
      "metadata": {
        "id": "ndGLPBL6-89e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Classify the emotion of this Text:\n",
        "\n",
        "\"{text}\"\n",
        "\n",
        "Only use one of the following classes: [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"].\n",
        "Classify the emotion using the \"EMOTION\" tag and provide a concise explanation using the \"EXPLANATION\" tag. Pay particular attention to sarcasm and irony when classifying.\n",
        "\n",
        "Here are some Examples:\n",
        "\n",
        "Text: \"I can't believe they lied to me again. I'm done with them.\"\n",
        "EMOTION: anger\n",
        "EXPLANATION: The speaker expresses frustration and a sense of betrayal, which are indicators of anger.\n",
        "\n",
        "Text: \"I just got promoted at work today! I'm so happy!\"\n",
        "EMOTION: joy\n",
        "EXPLANATION: The speaker is celebrating a positive personal achievement, showing signs of joy.\n",
        "\n",
        "Text: \"She didn’t show up again... I guess I should stop expecting anything.\"\n",
        "EMOTION: sadness\n",
        "EXPLANATION: The speaker feels let down and emotionally hurt, suggesting a mood of sadness.\n",
        "\n",
        "Text: \"Oh great, another meeting that could’ve been an email. Just what I needed today.\"\n",
        "EMOTION: disgust\n",
        "EXPLANATION: The sarcastic tone conveys annoyance and disdain, which are characteristic of disgust.\"\"\""
      ],
      "metadata": {
        "id": "yDOxngsG-9P-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n",
        "Now we can test the model and how it performs with the different prompting strategies."
      ],
      "metadata": {
        "id": "uUg1mXme_6_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Seriously? Another delay? This is so frustrating.\"\n",
        "\n",
        "zero_shot_messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": zero_shot_prompt.format(text=text)}]}]\n",
        "few_shot_messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": few_shot_prompt.format(text=text)}]}]\n",
        "\n",
        "zero_shot_out = llm(zero_shot_messages, max_new_tokens=50)\n",
        "few_shot_out = llm(few_shot_messages, max_new_tokens=50)"
      ],
      "metadata": {
        "id": "bCcHbdaG-CYv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the results."
      ],
      "metadata": {
        "id": "1_tXVnOhAgTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Text: {text}\\n\")\n",
        "print('# ZERO-SHOT')\n",
        "print(zero_shot_out[0]['generated_text'][1]['content'])\n",
        "print('\\n# FEW-SHOT')\n",
        "print(few_shot_out[0]['generated_text'][1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FApC2Sqb_JBG",
        "outputId": "90d2b2ac-cfb2-4002-9e37-61376e8623fc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Seriously? Another delay? This is so frustrating.\n",
            "\n",
            "# ZERO-SHOT\n",
            "Classify the emotion: **disgust**\n",
            "\n",
            "EXPLANATION: The phrase “Seriously? Another delay?” combined with the exclamation \"This is so frustrating\" creates a tone of deep disapproval and annoyance. The use of “Seriously?” subtly implies a\n",
            "\n",
            "# FEW-SHOT\n",
            "Text: \"It’s just… a really, really long wait. I'm starting to think I'll be sleeping on the job.\"\n",
            "\n",
            "EMOTION: frustration\n",
            "EXPLANATION: The speaker expresses a feeling of annoyance and helplessness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to add a **system prompt** to make it easier for the model to follow the instruction."
      ],
      "metadata": {
        "id": "JOjTaIzKBonP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n",
        "\n",
        "system_prompt = f\"\"\"You calssify texts into one of the following pre-defined categories: {categories}.\n",
        "You output the classification with the following format: CLASSIFICATION: <category>\n",
        "and provide a brief, concise explanation with the following format: EXPLANATION: <explanation>.\n",
        "The list of category is **exhaustive**.\"\"\"\n",
        "\n",
        "zero_shot_prompt = \"Text: \\\"{text}\\\"\"\n",
        "\n",
        "system_messages = [\n",
        "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
        "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": zero_shot_prompt.format(text=text)}]},\n",
        "]\n",
        "\n",
        "system_out = llm(system_messages, max_new_tokens=50)"
      ],
      "metadata": {
        "id": "oN3TgOJyBub3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the result."
      ],
      "metadata": {
        "id": "0B9o3FFjCz0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(system_out[0]['generated_text'][2]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O_UMEIDC1Va",
        "outputId": "44251109-84fd-45cc-f5ab-4028b8f441c5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASSIFICATION: anger\n",
            "EXPLANATION: The phrase expresses frustration and annoyance with a significant delay.\n"
          ]
        }
      ]
    }
  ]
}